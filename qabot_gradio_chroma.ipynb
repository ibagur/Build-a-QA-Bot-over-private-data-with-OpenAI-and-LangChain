{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Q&A Bot over private data with OpenAI and LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/pulse/build-qa-bot-over-private-data-openai-langchain-leo-wang/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "#llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')\n",
    "#llm = OpenAI(temperature=0, model='gpt-3.5-turbo')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Digestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "pdf_loader = DirectoryLoader('./Reports/', glob=\"**/*.pdf\")\n",
    "txt_loader = DirectoryLoader('./Reports/', glob=\"**/*.txt\")\n",
    "word_loader = DirectoryLoader('./Reports/', glob=\"**/*.docx\")\n",
    "\n",
    "loaders = [pdf_loader, txt_loader, word_loader]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(f\"Total number of documents: {len(documents)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ingested, it needs to be split into smaller chunks. By default, Tiktoken is used to count tokens for OpenAI LLMs.\n",
    "\n",
    "You can also use it to count tokens when splitting documents.\n",
    "\n",
    "Here we are splitting the text into 1k tokens with no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1239, which is longer than the specified 1000\n",
      "Created a chunk of size 1078, which is longer than the specified 1000\n",
      "Created a chunk of size 1036, which is longer than the specified 1000\n",
      "Created a chunk of size 1311, which is longer than the specified 1000\n",
      "Created a chunk of size 1019, which is longer than the specified 1000\n",
      "Created a chunk of size 1371, which is longer than the specified 1000\n",
      "Created a chunk of size 1019, which is longer than the specified 1000\n",
      "Created a chunk of size 1028, which is longer than the specified 1000\n",
      "Created a chunk of size 1895, which is longer than the specified 1000\n",
      "Created a chunk of size 1302, which is longer than the specified 1000\n",
      "Created a chunk of size 1381, which is longer than the specified 1000\n",
      "Created a chunk of size 1152, which is longer than the specified 1000\n",
      "Created a chunk of size 1038, which is longer than the specified 1000\n",
      "Created a chunk of size 1011, which is longer than the specified 1000\n",
      "Created a chunk of size 1637, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: chroma\n"
     ]
    }
   ],
   "source": [
    "persist_dir = \"chroma\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=persist_dir)\n",
    "vectorstore.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Retrieval Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain's chains are easily reusable components which can be linked together. It is simple a chain of actions that has been pre-built (pre-defined) into a single line of code. You don't need to call the GPT model, define the properties with prompt.\n",
    "\n",
    "This particular chain gives you the ability to chat over the documents and also remembers the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default OpenAI() LLM wrapper (less chatty)\n",
    "#qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)\n",
    "# Using the more conversational ChatOpenAI() wrapper\n",
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the QA Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is talking about earthquakes and their impact on various aspects of life, including infrastructure, services, and people with disabilities.\n"
     ]
    }
   ],
   "source": [
    "## Test QA chain\n",
    "user_message = \"What kind of disaster is the text talking about?\"\n",
    "history = []\n",
    "response = qa({\"question\": user_message, \"chat_history\": history})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the logic:\n",
    "\n",
    "1. Start a new variable \"chat_history\" with empty string\n",
    "2. Always pass the user question and history to the model\n",
    "3. Append the answer to the chat history\n",
    "4. Repeat\n",
    "\n",
    "It is literally three lines of code. I had a function only because of the front end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# Front end web app\n",
    "import gradio as gr\n",
    "demo = gr.Blocks()\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ðŸ¦œðŸ”— Ask TÃ¼rkiye Humanitarian Response Bot!\n",
    "        Start typing below to see the output.\n",
    "        \"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    def user(user_message, history):\n",
    "        # Format the list according to the expected input by ConversationalRetrievalChain\n",
    "        history = [(item[0], item[1]) for item in history]\n",
    "        # Get response from QA chain\n",
    "        response = qa({\"question\": user_message, \"chat_history\": history})\n",
    "        # Append user message and response to chat history\n",
    "        history.append((user_message, response[\"answer\"]))\n",
    "\n",
    "        return gr.update(value=\"\"), history\n",
    "    \n",
    "    msg.submit(user, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the reference to the text source, but not including it in the internal QA history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front end web app\n",
    "import gradio as gr\n",
    "demo = gr.Blocks()\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ðŸ¦œðŸ”— Ask TÃ¼rkiye Humanitarian Response Bot!\n",
    "        Start typing below to see the output.\n",
    "        \"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot()\n",
    "    #output = gr.Textbox()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    def user(user_message, history):\n",
    "        # Format the list according to the expected input by ConversationalRetrievalChain\n",
    "        history_qa = [(item[0], str.splitlines(item[1])[0]) for item in history] # internal history for the QA chain\n",
    "        history = [(item[0], item[1]) for item in history] # whole history to display in the Chatbot\n",
    "        # Get response from QA chain\n",
    "        response = qa({\"question\": user_message, \"chat_history\": history_qa})\n",
    "        # Get the source document reference\n",
    "        src = response['source_documents'][0].metadata['source']\n",
    "        # Append user message and response to chat history\n",
    "        history.append((user_message, response[\"answer\"] + '\\n' + \"Source: \" + src))\n",
    "\n",
    "        return gr.update(value=\"\"), history\n",
    "    \n",
    "    msg.submit(user, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chat memory buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# with chat memory (no explicitely defined)\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), vectorstore.as_retriever(), memory=memory)\n",
    "\n",
    "# Front end web app\n",
    "import gradio as gr\n",
    "demo = gr.Blocks()\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ðŸ¦œðŸ”— Ask TÃ¼rkiye Humanitarian Response Bot!\n",
    "        Start typing below to see the output.\n",
    "        \"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    def user(user_message, history):\n",
    "        # Format the list according to the expected input by ConversationalRetrievalChain\n",
    "        history = [(item[0], item[1]) for item in history]\n",
    "        # Get response from QA chain (history not used here, it is already buffered)\n",
    "        response = qa({\"question\": user_message})\n",
    "        # Keep the same ouput as before avoid error in Gradio, but explicit history is not used in QA chain\n",
    "        history.append((user_message, response[\"answer\"]))\n",
    "        return gr.update(value=\"\"), history\n",
    "    \n",
    "    msg.submit(user, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-agent-npQlq64W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Q&A Bot over private data with OpenAI and LangChain (using Vectara)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/pulse/build-qa-bot-over-private-data-openai-langchain-leo-wang/\n",
    "\n",
    "https://blog.langchain.dev/langchain-vectara-better-together/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up a Vectara account and create a corpus. After creating an API key for that corpus, we can set up the required arguments as environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = \"2280029605\"\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = \"1\"\n",
    "os.environ[\"VECTARA_API_KEY\"] = \"zwt_h-Z9pWOEjR_0R9Bo6LpePFCpRK0ZJl04RBJ_Xg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectara provides its own embeddings that are optimized for accurate retrieval, so we actually donâ€™t have to use (or pay for) an additional embedding model. Document has been already uploaded in Vectara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Vectara\n",
    "\n",
    "# vectara = Vectara(\n",
    "#     vectara_customer_id=customer_id, \n",
    "#     vectara_corpus_id=corpus_id, \n",
    "#     vectara_api_key=api_key\n",
    "# )\n",
    "\n",
    "# Default parameters taken from environment variables\n",
    "vectara = Vectara()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can simply use Vectara.from_documents() to upload the documents into Vectaraâ€™s index for this corpus, and use that as a retriever in the chain. Vectara takes the source documents and automatically chunks it in an optimized manner and creates the embeddings, so we donâ€™t even have to use the TextSplitter (and decide on chunk size), nor do we need to call (or pay for) OpenAIEmbeddings. Since Vectara has its own internal vector storage, we donâ€™t need to use FAISS or any other commercial vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "pdf_loader = DirectoryLoader('./Reports/', glob=\"**/*.pdf\")\n",
    "txt_loader = DirectoryLoader('./Reports/', glob=\"**/*.txt\")\n",
    "word_loader = DirectoryLoader('./Reports/', glob=\"**/*.docx\")\n",
    "\n",
    "loaders = [pdf_loader, txt_loader, word_loader]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(f\"Total number of documents: {len(documents)}\")\n",
    "\n",
    "vectara  = Vectara.from_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the QA Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), vectara.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is talking about earthquakes that occurred in Turkey.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"What kind of disaster is the text talking about?\"\n",
    "history = []\n",
    "response = qa({\"question\": user_message, \"chat_history\": history})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Gradio bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/.local/share/virtualenvs/auto-agent-npQlq64W/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# Front end web app\n",
    "import gradio as gr\n",
    "demo = gr.Blocks()\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ðŸ¦œðŸ”— Ask TÃ¼rkiye Humanitarian Response Bot!\n",
    "        Start typing below to see the output.\n",
    "        \"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    def user(user_message, history):\n",
    "        # Format the list according to the expected input by ConversationalRetrievalChain\n",
    "        history = [(item[0], item[1]) for item in history]\n",
    "        # Get response from QA chain\n",
    "        response = qa({\"question\": user_message, \"chat_history\": history})\n",
    "        # Append user message and response to chat history\n",
    "        history.append((user_message, response[\"answer\"]))\n",
    "\n",
    "        return gr.update(value=\"\"), history\n",
    "    \n",
    "    msg.submit(user, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-agent-npQlq64W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
